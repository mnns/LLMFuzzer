<div align="center">

# ğŸ§  LLMFuzzer - Fuzzing Framework for Large Language Models ğŸ§ 

![LLMFuzzer-shell](https://github.com/mnns/LLMFuzzer/assets/1796080/71b006df-706c-43f6-acd1-49646dbcb0e5)

![Version](https://img.shields.io/badge/version-1.0.0-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Stars](https://img.shields.io/github/stars/LLMFuzzer/LLMFuzzer)
![Forks](https://img.shields.io/github/forks/LLMFuzzer/LLMFuzzer)
![Issues](https://img.shields.io/github/issues/LLMFuzzer/LLMFuzzer)

</div>

LLMFuzzer is the first open-source fuzzing framework specifically designed for Large Language Models (LLMs), especially for their integrations in applications via LLM APIs. ğŸš€ğŸ’¥

# ğŸ¯ Who is this for?

If you're a security enthusiast, a pentester, or a cybersec researcher who loves to find and exploit vulnerabilities in AI systems, LLMFuzzer is the perfect tool for you. It's built to make your testing process streamlined and efficient. ğŸ•µï¸â€â™€ï¸

# ğŸŒŸ Features

- Robust fuzzing for LLMs ğŸ§ª
- API integration testing ğŸ› ï¸
- Wide range of fuzzing strategies ğŸŒ€
- Modular architecture for easy extendability ğŸ“š

## ğŸš€ Get Started

1. Clone the repo
```bash
git clone https://github.com/LLMFuzzer/LLMFuzzer.git
```

2. Navigate to the project directory
```bash
cd LLMFuzzer
```

3. Install dependencies
```bash
pip install -r requirements.txt
```

4. Run LLMFuzzer
```bash
python main.py
```

## ğŸ“š Documentation
We are working on full documentation. It will cover detailed information about the architecture, different fuzzing strategies, examples, and how to extend the tool.

## ğŸ¤ Contributing
We welcome all contributors who are passionate about improving LLMFuzzer. See our contributing guidelines for ways to get started. ğŸ¤—

## ğŸ’¼ License
LLMFuzzer is licensed under the MIT License. See the LICENSE file for more details.

## ğŸ© Acknowledgments
LLMFuzzer couldn't exist without the community. We appreciate all our contributors and supporters. Let's make AI safer together! ğŸ’–

